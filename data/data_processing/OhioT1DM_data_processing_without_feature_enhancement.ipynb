{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **OHIOT1DM DATA PROCESSING**   \n",
    "- **Without feature enhancement**  \n",
    " \n",
    "- **2:1:2 hypo:eu:hyper sampling ratio**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CONTENTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Requirements & Encironment](##1.-Requirements-&-Environment)  \n",
    "[2. Read n OHIO-T1DM Data](##2.-Read-n-OHIO-T1DM-Data)  \n",
    "[3. Initial Data Processing](##3.-OHIO-T1DM-Data-Initial-Processing)  \n",
    "[4. OHIO T1DM Data Processing - No Undersampling](##4.-OHIO-T1DM-Data-Processing---No-Undersampling)  \n",
    "[5. OHIO T1DM Data Processing - Hypo Oversamplin and  Eu/Hyper Undersampling](##5.-OHIO-T1DM-Data-Processing---Hypo-Oversampling-and-Eu/Hyper-Undersampling)  \n",
    "[6. OHIO T1DM Validation Data Processing](##6.-OHIO-T1DM-Validation-Data-Processing)  \n",
    "[7. OHIO T1DM Test Data Processing](##7.-OHIO-T1DM-Test-Data-Processing)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Requirements & Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import sys\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data_processing_modules import *\n",
    "from data_processing_parameters import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#CONTENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Read in OHIO-T1DM Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved file file paths\n",
    "\n",
    "ohio_training_directory_no_undersampling = '../processed_data/ohio/training/no_undersampling'\n",
    "os.makedirs(ohio_training_directory_no_undersampling, exist_ok=True)\n",
    "\n",
    "ohio_training_directory_undersampling = '../processed_data/ohio/training/over_and_under_sampling'\n",
    "os.makedirs(ohio_training_directory_undersampling, exist_ok=True)\n",
    "\n",
    "ohio_validation_directory = '../processed_data/ohio/validation'\n",
    "os.makedirs(ohio_validation_directory, exist_ok=True)\n",
    "\n",
    "ohio_test_directory = '../processed_data/ohio/testing'\n",
    "os.makedirs(ohio_test_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohio_training_data = get_ohio_data('training', 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([584, 575, 563, 559, 540, 596, 588, 570, 591, 567, 552, 544])\n"
     ]
    }
   ],
   "source": [
    "print(ohio_training_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohio_test_data = get_ohio_data('test', 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([552, 540, 559, 544, 588, 570, 563, 596, 591, 584, 567, 575])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohio_test_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(ohio_training_data.keys() == ohio_test_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#CONTENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirms that data for each patient ID is present in both the training and testing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. OHIO-T1DM Data Initial Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 584, Number of Glucose Measurements: 12150\n",
      "Patient ID: 575, Number of Glucose Measurements: 11866\n",
      "Patient ID: 563, Number of Glucose Measurements: 12124\n",
      "Patient ID: 559, Number of Glucose Measurements: 10796\n",
      "Patient ID: 540, Number of Glucose Measurements: 11947\n",
      "Patient ID: 596, Number of Glucose Measurements: 10877\n",
      "Patient ID: 588, Number of Glucose Measurements: 12640\n",
      "Patient ID: 570, Number of Glucose Measurements: 10982\n",
      "Patient ID: 591, Number of Glucose Measurements: 10847\n",
      "Patient ID: 567, Number of Glucose Measurements: 10858\n",
      "Patient ID: 552, Number of Glucose Measurements: 9080\n",
      "Patient ID: 544, Number of Glucose Measurements: 10623\n"
     ]
    }
   ],
   "source": [
    "for ptid, df in ohio_training_data.items():\n",
    "    print(f\"Patient ID: {ptid}, Number of Glucose Measurements: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohio_training_dict = {}\n",
    "ohio_validation_dict = {}\n",
    "\n",
    "for ptid, df in ohio_training_data.items():\n",
    "    train, test = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "    ohio_training_dict[ptid] = train\n",
    "    ohio_validation_dict[ptid] = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#CONTENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. OHIO T1DM Data Processing - No Undersampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ptid, df in ohio_training_dict.items():\n",
    "    df = df.copy()\n",
    "    df['real_value_flag'] = 1\n",
    "    df['TimeDiff'] = df['timestamp'].diff().dt.total_seconds()\n",
    "\n",
    "    # Identify rows where TimeDiff is around 600 seconds (10 min)\n",
    "    mask = (df['TimeDiff'] > 595) & (df['TimeDiff'] < 605)\n",
    "    insert_rows = df[mask].copy()\n",
    "\n",
    "    if not insert_rows.empty:\n",
    "        # Modify new rows: set `real_value_flag = 0`, shift `DateTime`, and set `GlucoseValue = NaN`\n",
    "        insert_rows['real_value_flag'] = 0\n",
    "        insert_rows['timestamp'] -= pd.to_timedelta(5, unit='m')\n",
    "        insert_rows['GlucoseValue'] = np.nan\n",
    "\n",
    "        # Append new rows to the dataframe and sort\n",
    "        df = pd.concat([df, insert_rows]).sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Convert 'value' column to numeric before interpolation\n",
    "    df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "    df['GlucoseValue'] = df['value'].interpolate(method='linear')\n",
    "\n",
    "    df['Hour'] = df['timestamp'].dt.hour\n",
    "    df['Minute'] = df['timestamp'].dt.minute\n",
    "    df['TimeDiff'] = df['timestamp'].diff().dt.total_seconds()\n",
    "    df['TimeDiffFlag'] = df['TimeDiff'].apply(lambda x: 0 if x < 295 or x > 305 else 1)\n",
    "    df['RollingTimeDiffFlag'] = df['TimeDiffFlag'].rolling(window=96).sum()\n",
    "\n",
    "    # Drop first 96 rows due to NaN values\n",
    "    # df = df.iloc[95:].reset_index(drop=True)\n",
    "\n",
    "    # drop columns\n",
    "    df = df.drop(columns=['timestamp', 'value', 'TimeDiff', 'TimeDiffFlag', 'real_value_flag'])\n",
    "\n",
    "    ohio_training_dict[ptid] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace-BG normalisation metrics\n",
    "\n",
    "normalisation_mean = 152.91051040286524\n",
    "\n",
    "normalisation_std = 70.27050122812615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ptid, df in ohio_training_dict.items():\n",
    "    df['GlucoseValue'] = (df['GlucoseValue'] - normalisation_mean) / normalisation_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptid_training_slice_dict = {}\n",
    "\n",
    "\n",
    "for ptid, df in ohio_training_dict.items():\n",
    "    rolling_flag_array = df[\"RollingTimeDiffFlag\"].to_numpy()  # Convert to NumPy array for fast indexing\n",
    "    num_rows = len(df)\n",
    "    starting_index = 0\n",
    "\n",
    "    slice_list = []\n",
    "\n",
    "    while starting_index + slice_size <= num_rows:\n",
    "        if rolling_flag_array[starting_index + slice_size - 1] == slice_size:  # Use precomputed array\n",
    "            slice_df = df.iloc[starting_index:starting_index + slice_size].copy()\n",
    "            slice_df = slice_df.drop(columns='RollingTimeDiffFlag')\n",
    "            slice_list.append(slice_df)\n",
    "            starting_index += 1\n",
    "\n",
    "        else:\n",
    "            starting_index += 1\n",
    "        \n",
    "    ptid_training_slice_dict[ptid] = slice_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for ptid, slice_list in ptid_training_slice_dict.items():\n",
    "\n",
    "\n",
    "    ptid_count = 0\n",
    "\n",
    "    encoder_dir = os.path.join(ohio_training_directory_no_undersampling, f'ohio_training_{ptid}', 'EncoderSlices')\n",
    "    os.makedirs(encoder_dir, exist_ok=True)\n",
    "    decoder_dir = os.path.join(ohio_training_directory_no_undersampling, f'ohio_training_{ptid}', 'DecoderSlices')\n",
    "    os.makedirs(decoder_dir, exist_ok=True)\n",
    "    target_dir = os.path.join(ohio_training_directory_no_undersampling, f'ohio_training_{ptid}', 'TargetSlices')\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    for i, slice_df in enumerate(slice_list):\n",
    "        # Replace all instances of 'slice' with 'slice_df'\n",
    "        encoder_input = slice_df.iloc[:encoder_input_size]\n",
    "        target = slice_df.iloc[encoder_input_size:]['GlucoseValue']\n",
    "\n",
    "        decoder_input = slice_df.iloc[-decoder_input_size:].copy().reset_index(drop=True)\n",
    "        decoder_input.loc[decoder_input.index[start_token_size:], 'GlucoseValue'] = 0\n",
    "\n",
    "        encoder_path = os.path.join(encoder_dir, f'{ptid_count}.pt')\n",
    "        decoder_path = os.path.join(decoder_dir, f'{ptid_count}.pt')\n",
    "        target_path = os.path.join(target_dir, f'{ptid_count}.pt')\n",
    "\n",
    "        torch.save(torch.tensor(encoder_input.values, dtype=torch.float32), encoder_path)\n",
    "        torch.save(torch.tensor(decoder_input.values, dtype=torch.float32), decoder_path)\n",
    "        torch.save(torch.tensor(target.values, dtype=torch.float32), target_path)\n",
    "\n",
    "        ptid_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Encoder Shape: (72, 3)\n",
      "    GlucoseValue  Hour  Minute\n",
      "67      0.157811  23.0    59.0\n",
      "68      0.129350   0.0     4.0\n",
      "69      0.100889   0.0     9.0\n",
      "70      0.072427   0.0    14.0\n",
      "71      0.043966   0.0    19.0\n",
      "\n",
      " Decoder Shape: (36, 3)\n",
      "    GlucoseValue  Hour  Minute\n",
      "6       0.172042  23.0    54.0\n",
      "7       0.157811  23.0    59.0\n",
      "8       0.129350   0.0     4.0\n",
      "9       0.100889   0.0     9.0\n",
      "10      0.072427   0.0    14.0\n",
      "11      0.043966   0.0    19.0\n",
      "12      0.000000   0.0    24.0\n",
      "13      0.000000   0.0    29.0\n",
      "14      0.000000   0.0    34.0\n",
      "15      0.000000   0.0    39.0\n",
      "16      0.000000   0.0    44.0\n",
      "17      0.000000   0.0    49.0\n",
      "18      0.000000   0.0    54.0\n",
      "19      0.000000   0.0    59.0\n",
      "20      0.000000   1.0     4.0\n",
      "21      0.000000   1.0     9.0\n",
      "22      0.000000   1.0    14.0\n",
      "23      0.000000   1.0    19.0\n",
      "24      0.000000   1.0    24.0\n",
      "25      0.000000   1.0    29.0\n",
      "26      0.000000   1.0    34.0\n",
      "27      0.000000   1.0    39.0\n",
      "28      0.000000   1.0    44.0\n",
      "29      0.000000   1.0    49.0\n",
      "30      0.000000   1.0    54.0\n",
      "31      0.000000   1.0    59.0\n",
      "32      0.000000   2.0     4.0\n",
      "33      0.000000   2.0     9.0\n",
      "34      0.000000   2.0    14.0\n",
      "35      0.000000   2.0    19.0\n",
      "\n",
      " Target Shape: (24, 1)\n",
      "    GlucoseValue\n",
      "19     -0.511033\n",
      "20     -0.582186\n",
      "21     -0.667570\n",
      "22     -0.710263\n",
      "23     -0.738724\n"
     ]
    }
   ],
   "source": [
    "# load first file for ohio_training_544\n",
    "encoder_path = get_first_file(os.path.join(ohio_training_directory_no_undersampling, 'ohio_training_544', 'EncoderSlices'))\n",
    "decoder_path = get_first_file(os.path.join(ohio_training_directory_no_undersampling, 'ohio_training_544', 'DecoderSlices'))\n",
    "target_path = get_first_file(os.path.join(ohio_training_directory_no_undersampling, 'ohio_training_544', 'TargetSlices'))\n",
    "\n",
    "encoder_tensor = torch.load(encoder_path)\n",
    "decoder_tensor = torch.load(decoder_path)\n",
    "target_tensor = torch.load(target_path)\n",
    "\n",
    "encoder_df = pd.DataFrame(encoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "decoder_df = pd.DataFrame(decoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "target_df = pd.DataFrame(target_tensor.numpy(), columns=[\"GlucoseValue\"])\n",
    "\n",
    "print(f\"\\n Encoder Shape: {encoder_df.shape}\")\n",
    "print(encoder_df.tail())\n",
    "print(f\"\\n Decoder Shape: {decoder_df.shape}\")\n",
    "print(decoder_df.tail(30))\n",
    "print(f\"\\n Target Shape: {target_df.shape}\")\n",
    "print(target_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#CONTENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. OHIO T1DM Data Processing - Hypo OverSampling and Eu/Hyper Undersampling**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 584, Hypo: 97, Hyper: 3366, EU: 2138\n",
      "Patient ID: 575, Hypo: 641, Hyper: 1140, EU: 2725\n",
      "Patient ID: 563, Hypo: 357, Hyper: 2411, EU: 5497\n",
      "Patient ID: 559, Hypo: 270, Hyper: 2635, EU: 2584\n",
      "Patient ID: 540, Hypo: 847, Hyper: 2143, EU: 4703\n",
      "Patient ID: 596, Hypo: 146, Hyper: 2390, EU: 4343\n",
      "Patient ID: 588, Hypo: 141, Hyper: 4332, EU: 4781\n",
      "Patient ID: 570, Hypo: 190, Hyper: 4523, EU: 2711\n",
      "Patient ID: 591, Hypo: 279, Hyper: 3004, EU: 3398\n",
      "Patient ID: 567, Hypo: 402, Hyper: 1659, EU: 2508\n",
      "Patient ID: 552, Hypo: 329, Hyper: 1313, EU: 2732\n",
      "Patient ID: 544, Hypo: 189, Hyper: 3251, EU: 3571\n"
     ]
    }
   ],
   "source": [
    "ptid_hypo_training_slice_dict = {}\n",
    "ptid_hyper_training_slice_dict = {}\n",
    "ptid_eu_training_slice_dict = {}\n",
    "\n",
    "for ptid, df_list in ptid_training_slice_dict.items():\n",
    "    hypo_list = []\n",
    "    hyper_list = []\n",
    "    eu_list = []\n",
    "\n",
    "    normalised_hypo_threshold = (70 - normalisation_mean) / normalisation_std\n",
    "    normalised_hyper_threshold = (180 - normalisation_mean) / normalisation_std\n",
    "\n",
    "    threshold_count = 6\n",
    "\n",
    "    for slice in df_list:\n",
    "\n",
    "        target_slice = slice.iloc[-target_size:]['GlucoseValue'].values\n",
    "\n",
    "        hypo_count = np.sum(target_slice < normalised_hypo_threshold)\n",
    "        hyper_count = np.sum(target_slice > normalised_hyper_threshold)\n",
    "        eu_count = target_size - hypo_count - hyper_count\n",
    "\n",
    "        if hypo_count >= threshold_count:\n",
    "            hypo_list.append(slice)\n",
    "        elif hyper_count >= threshold_count:\n",
    "            hyper_list.append(slice)\n",
    "        else:\n",
    "            eu_list.append(slice)\n",
    "\n",
    "    \n",
    "    print(f\"Patient ID: {ptid}, Hypo: {len(hypo_list)}, Hyper: {len(hyper_list)}, EU: {len(eu_list)}\")\n",
    "\n",
    "    ptid_hypo_training_slice_dict[ptid] = hypo_list\n",
    "    ptid_hyper_training_slice_dict[ptid] = hyper_list\n",
    "    ptid_eu_training_slice_dict[ptid] = eu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 584, Hypo: 194\n",
      "Patient ID: 575, Hypo: 1282\n",
      "Patient ID: 563, Hypo: 714\n",
      "Patient ID: 559, Hypo: 540\n",
      "Patient ID: 540, Hypo: 1694\n",
      "Patient ID: 596, Hypo: 292\n",
      "Patient ID: 588, Hypo: 282\n",
      "Patient ID: 570, Hypo: 380\n",
      "Patient ID: 591, Hypo: 558\n",
      "Patient ID: 567, Hypo: 804\n",
      "Patient ID: 552, Hypo: 658\n",
      "Patient ID: 544, Hypo: 378\n"
     ]
    }
   ],
   "source": [
    "for ptid, slice_list in ptid_hypo_training_slice_dict.items():\n",
    "    #duplicate the hypo slices\n",
    "    slice_list = slice_list + slice_list\n",
    "    ptid_hypo_training_slice_dict[ptid] = slice_list\n",
    "    print(f\"Patient ID: {ptid}, Hypo: {len(slice_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 584, Hyper: 194\n",
      "Patient ID: 575, Hyper: 1140\n",
      "Patient ID: 563, Hyper: 714\n",
      "Patient ID: 559, Hyper: 540\n",
      "Patient ID: 540, Hyper: 1694\n",
      "Patient ID: 596, Hyper: 292\n",
      "Patient ID: 588, Hyper: 282\n",
      "Patient ID: 570, Hyper: 380\n",
      "Patient ID: 591, Hyper: 558\n",
      "Patient ID: 567, Hyper: 804\n",
      "Patient ID: 552, Hyper: 658\n",
      "Patient ID: 544, Hyper: 378\n"
     ]
    }
   ],
   "source": [
    "for ptid, slice_list in ptid_hyper_training_slice_dict.items():\n",
    "\n",
    "    target_size = len(ptid_hypo_training_slice_dict[ptid])\n",
    "\n",
    "    hyper_dict = {idx: slice for idx, slice in enumerate(slice_list)}\n",
    "    hyper_dict = undersample_dict(hyper_dict, target_size)\n",
    "\n",
    "    ptid_hyper_training_slice_dict[ptid] = list(hyper_dict.values())\n",
    "\n",
    "    print(f\"Patient ID: {ptid}, Hyper: {len(ptid_hyper_training_slice_dict[ptid])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 584, EU: 194\n",
      "Patient ID: 575, EU: 1282\n",
      "Patient ID: 563, EU: 714\n",
      "Patient ID: 559, EU: 540\n",
      "Patient ID: 540, EU: 1694\n",
      "Patient ID: 596, EU: 292\n",
      "Patient ID: 588, EU: 282\n",
      "Patient ID: 570, EU: 380\n",
      "Patient ID: 591, EU: 558\n",
      "Patient ID: 567, EU: 804\n",
      "Patient ID: 552, EU: 658\n",
      "Patient ID: 544, EU: 378\n"
     ]
    }
   ],
   "source": [
    "for ptid, slice_list in ptid_eu_training_slice_dict.items():\n",
    "\n",
    "    target_size = len(ptid_hypo_training_slice_dict[ptid])\n",
    "\n",
    "    eu_dict = {idx: slice for idx, slice in enumerate(slice_list)}\n",
    "    eu_dict = undersample_dict(eu_dict, target_size)\n",
    "\n",
    "    ptid_eu_training_slice_dict[ptid] = list(eu_dict.values())\n",
    "\n",
    "    print(f\"Patient ID: {ptid}, EU: {len(ptid_eu_training_slice_dict[ptid])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 584, Total Slices: 582\n",
      "Patient ID: 575, Total Slices: 3704\n",
      "Patient ID: 563, Total Slices: 2142\n",
      "Patient ID: 559, Total Slices: 1620\n",
      "Patient ID: 540, Total Slices: 5082\n",
      "Patient ID: 596, Total Slices: 876\n",
      "Patient ID: 588, Total Slices: 846\n",
      "Patient ID: 570, Total Slices: 1140\n",
      "Patient ID: 591, Total Slices: 1674\n",
      "Patient ID: 567, Total Slices: 2412\n",
      "Patient ID: 552, Total Slices: 1974\n",
      "Patient ID: 544, Total Slices: 1134\n"
     ]
    }
   ],
   "source": [
    "undersampled_training_dict = {}\n",
    "\n",
    "for ptid, slice_list in ptid_hypo_training_slice_dict.items():\n",
    "    hypo_slices = slice_list\n",
    "    hyper_slices = ptid_hyper_training_slice_dict[ptid]\n",
    "    eu_slices = ptid_eu_training_slice_dict[ptid]\n",
    "\n",
    "    final_slices = hypo_slices + hyper_slices + eu_slices\n",
    "\n",
    "    np.random.shuffle(final_slices)\n",
    "    undersampled_training_dict[ptid] = final_slices\n",
    "\n",
    "    print(f\"Patient ID: {ptid}, Total Slices: {len(final_slices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ptid, slice_list in undersampled_training_dict.items():\n",
    "    \n",
    "    ptid_count = 0\n",
    "\n",
    "    encoder_dir = os.path.join(ohio_training_directory_undersampling, f'ohio_training_{ptid}', 'EncoderSlices')\n",
    "    os.makedirs(encoder_dir, exist_ok=True)\n",
    "    decoder_dir = os.path.join(ohio_training_directory_undersampling, f'ohio_training_{ptid}', 'DecoderSlices')\n",
    "    os.makedirs(decoder_dir, exist_ok=True)\n",
    "    target_dir = os.path.join(ohio_training_directory_undersampling, f'ohio_training_{ptid}', 'TargetSlices')\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    for i, slice in enumerate(slice_list):\n",
    "\n",
    "        encoder_input = slice.iloc[:encoder_input_size]\n",
    "        target = slice.iloc[encoder_input_size:]['GlucoseValue']\n",
    "\n",
    "        decoder_input = slice_df.iloc[-decoder_input_size:].copy().reset_index(drop=True)\n",
    "        decoder_input.loc[decoder_input.index[start_token_size:], 'GlucoseValue'] = 0\n",
    "\n",
    "        encoder_path = os.path.join(encoder_dir, f'{ptid_count}.pt')\n",
    "        decoder_path = os.path.join(decoder_dir, f'{ptid_count}.pt')\n",
    "        target_path = os.path.join(target_dir, f'{ptid_count}.pt')\n",
    "\n",
    "        torch.save(torch.tensor(encoder_input.values, dtype=torch.float32), encoder_path)\n",
    "        torch.save(torch.tensor(decoder_input.values, dtype=torch.float32), decoder_path)\n",
    "        torch.save(torch.tensor(target.values, dtype=torch.float32), target_path)\n",
    "\n",
    "        ptid_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Encoder Shape: (72, 3)\n",
      "    GlucoseValue  Hour  Minute\n",
      "67      0.029735  19.0    56.0\n",
      "68      0.072427  20.0     1.0\n",
      "69      0.058196  20.0     6.0\n",
      "70      0.015504  20.0    11.0\n",
      "71      0.001274  20.0    16.0\n",
      "\n",
      " Decoder Shape: (36, 3)\n",
      "    GlucoseValue  Hour  Minute\n",
      "6      -0.141034  23.0    19.0\n",
      "7      -0.141034  23.0    24.0\n",
      "8      -0.126803  23.0    29.0\n",
      "9      -0.126803  23.0    34.0\n",
      "10     -0.126803  23.0    39.0\n",
      "11     -0.112572  23.0    44.0\n",
      "12      0.000000  23.0    49.0\n",
      "13      0.000000  23.0    54.0\n",
      "14      0.000000  23.0    59.0\n",
      "15      0.000000   0.0     4.0\n",
      "16      0.000000   0.0     9.0\n",
      "17      0.000000   0.0    14.0\n",
      "18      0.000000   0.0    19.0\n",
      "19      0.000000   0.0    24.0\n",
      "20      0.000000   0.0    29.0\n",
      "21      0.000000   0.0    34.0\n",
      "22      0.000000   0.0    39.0\n",
      "23      0.000000   0.0    44.0\n",
      "24      0.000000   0.0    49.0\n",
      "25      0.000000   0.0    54.0\n",
      "26      0.000000   0.0    59.0\n",
      "27      0.000000   1.0     4.0\n",
      "28      0.000000   1.0     9.0\n",
      "29      0.000000   1.0    14.0\n",
      "30      0.000000   1.0    19.0\n",
      "31      0.000000   1.0    24.0\n",
      "32      0.000000   1.0    29.0\n",
      "33      0.000000   1.0    34.0\n",
      "34      0.000000   1.0    39.0\n",
      "35      0.000000   1.0    44.0\n",
      "\n",
      " Target Shape: (24, 1)\n",
      "    GlucoseValue\n",
      "19     -0.980646\n",
      "20     -1.009108\n",
      "21     -1.023339\n",
      "22     -1.094492\n",
      "23     -1.151415\n"
     ]
    }
   ],
   "source": [
    "# load first file for ohio_training_559\n",
    "encoder_path = get_first_file(os.path.join(ohio_training_directory_undersampling, 'ohio_training_540', 'EncoderSlices'))\n",
    "decoder_path = get_first_file(os.path.join(ohio_training_directory_undersampling, 'ohio_training_540', 'DecoderSlices'))\n",
    "target_path = get_first_file(os.path.join(ohio_training_directory_undersampling, 'ohio_training_540', 'TargetSlices'))\n",
    "\n",
    "encoder_tensor = torch.load(encoder_path)\n",
    "decoder_tensor = torch.load(decoder_path)\n",
    "target_tensor = torch.load(target_path)\n",
    "\n",
    "encoder_df = pd.DataFrame(encoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "decoder_df = pd.DataFrame(decoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "target_df = pd.DataFrame(target_tensor.numpy(), columns=[\"GlucoseValue\"])\n",
    "\n",
    "print(f\"\\n Encoder Shape: {encoder_df.shape}\")\n",
    "print(encoder_df.tail())\n",
    "print(f\"\\n Decoder Shape: {decoder_df.shape}\")\n",
    "print(decoder_df.tail(30))\n",
    "print(f\"\\n Target Shape: {target_df.shape}\")\n",
    "print(target_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#CONTENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. OHIO T1DM Validation Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ptid, df in ohio_validation_dict.items():\n",
    "    df = df.copy()\n",
    "    df['real_value_flag'] = 1\n",
    "    df['TimeDiff'] = df['timestamp'].diff().dt.total_seconds()\n",
    "\n",
    "    # Identify rows where TimeDiff is around 600 seconds (10 min)\n",
    "    mask = (df['TimeDiff'] > 595) & (df['TimeDiff'] < 605)\n",
    "    insert_rows = df[mask].copy()\n",
    "\n",
    "    if not insert_rows.empty:\n",
    "        # Modify new rows: set `real_value_flag = 0`, shift `DateTime`, and set `GlucoseValue = NaN`\n",
    "        insert_rows['real_value_flag'] = 0\n",
    "        insert_rows['timestamp'] -= pd.to_timedelta(5, unit='m')\n",
    "        insert_rows['GlucoseValue'] = np.nan\n",
    "\n",
    "        # Append new rows to the dataframe and sort\n",
    "        df = pd.concat([df, insert_rows]).sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Convert 'value' column to numeric before interpolation\n",
    "    df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "    df['GlucoseValue'] = df['value'].interpolate(method='linear')\n",
    "\n",
    "    df['Hour'] = df['timestamp'].dt.hour\n",
    "    df['Minute'] = df['timestamp'].dt.minute\n",
    "    df['TimeDiff'] = df['timestamp'].diff().dt.total_seconds()\n",
    "    df['TimeDiffFlag'] = df['TimeDiff'].apply(lambda x: 0 if x < 295 or x > 305 else 1)\n",
    "    df['RollingTimeDiffFlag'] = df['TimeDiffFlag'].rolling(window=96).sum()\n",
    "\n",
    "    # Drop first 96 rows due to NaN values\n",
    "    # df = df.iloc[95:].reset_index(drop=True)\n",
    "\n",
    "    # drop columns\n",
    "    df = df.drop(columns=['timestamp', 'value', 'TimeDiff', 'TimeDiffFlag', 'real_value_flag'])\n",
    "\n",
    "    ohio_validation_dict[ptid] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ptid, df in ohio_validation_dict.items():\n",
    "    df['GlucoseValue'] = (df['GlucoseValue'] - normalisation_mean) / normalisation_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptid_validation_slice_dict = {}\n",
    "\n",
    "\n",
    "for ptid, df in ohio_validation_dict.items():\n",
    "    rolling_flag_array = df[\"RollingTimeDiffFlag\"].to_numpy()  # Convert to NumPy array for fast indexing\n",
    "    num_rows = len(df)\n",
    "    starting_index = 0\n",
    "\n",
    "    slice_list = []\n",
    "\n",
    "    while starting_index + slice_size <= num_rows:\n",
    "        if rolling_flag_array[starting_index + slice_size - 1] == slice_size:  # Use precomputed array\n",
    "            slice_df = df.iloc[starting_index:starting_index + slice_size].copy()\n",
    "            slice_df = slice_df.drop(columns='RollingTimeDiffFlag')\n",
    "            slice_list.append(slice_df)\n",
    "            starting_index += 1\n",
    "\n",
    "        else:\n",
    "            starting_index += 1\n",
    "        \n",
    "    ptid_validation_slice_dict[ptid] = slice_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ptid, slice_list in ptid_validation_slice_dict.items():\n",
    "\n",
    "\n",
    "    ptid_count = 0\n",
    "\n",
    "    encoder_dir = os.path.join(ohio_validation_directory, f'ohio_validation_{ptid}', 'EncoderSlices')\n",
    "    os.makedirs(encoder_dir, exist_ok=True)\n",
    "    decoder_dir = os.path.join(ohio_validation_directory, f'ohio_validation_{ptid}', 'DecoderSlices')\n",
    "    os.makedirs(decoder_dir, exist_ok=True)\n",
    "    target_dir = os.path.join(ohio_validation_directory, f'ohio_validation_{ptid}', 'TargetSlices')\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    for i, slice_df in enumerate(slice_list):\n",
    "        # Replace all instances of 'slice' with 'slice_df'\n",
    "        encoder_input = slice.iloc[:encoder_input_size]\n",
    "        target = slice.iloc[encoder_input_size:]['GlucoseValue']\n",
    "\n",
    "        decoder_input = slice_df.iloc[-decoder_input_size:].copy().reset_index(drop=True)\n",
    "        decoder_input.loc[decoder_input.index[start_token_size:], 'GlucoseValue'] = 0\n",
    "\n",
    "        encoder_path = os.path.join(encoder_dir, f'{ptid_count}.pt')\n",
    "        decoder_path = os.path.join(decoder_dir, f'{ptid_count}.pt')\n",
    "        target_path = os.path.join(target_dir, f'{ptid_count}.pt')\n",
    "\n",
    "        torch.save(torch.tensor(encoder_input.values, dtype=torch.float32), encoder_path)\n",
    "        torch.save(torch.tensor(decoder_input.values, dtype=torch.float32), decoder_path)\n",
    "        torch.save(torch.tensor(target.values, dtype=torch.float32), target_path)\n",
    "\n",
    "        ptid_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Encoder Shape: (72, 3)\n",
      "    GlucoseValue  Hour  Minute\n",
      "67     -1.108723   6.0     4.0\n",
      "68     -1.151415   6.0     9.0\n",
      "69     -1.108723   6.0    14.0\n",
      "70     -1.108723   6.0    19.0\n",
      "71     -1.208338   6.0    24.0\n",
      "\n",
      " Decoder Shape: (36, 3)\n",
      "    GlucoseValue  Hour  Minute\n",
      "6       1.353192  21.0    50.0\n",
      "7       1.367423  21.0    55.0\n",
      "8       1.381654  22.0     0.0\n",
      "9       1.410115  22.0     5.0\n",
      "10      1.424346  22.0    10.0\n",
      "11      1.424346  22.0    15.0\n",
      "12      0.000000  22.0    20.0\n",
      "13      0.000000  22.0    25.0\n",
      "14      0.000000  22.0    30.0\n",
      "15      0.000000  22.0    35.0\n",
      "16      0.000000  22.0    40.0\n",
      "17      0.000000  22.0    45.0\n",
      "18      0.000000  22.0    50.0\n",
      "19      0.000000  22.0    55.0\n",
      "20      0.000000  23.0     0.0\n",
      "21      0.000000  23.0     5.0\n",
      "22      0.000000  23.0    10.0\n",
      "23      0.000000  23.0    15.0\n",
      "24      0.000000  23.0    20.0\n",
      "25      0.000000  23.0    25.0\n",
      "26      0.000000  23.0    30.0\n",
      "27      0.000000  23.0    35.0\n",
      "28      0.000000  23.0    40.0\n",
      "29      0.000000  23.0    45.0\n",
      "30      0.000000  23.0    50.0\n",
      "31      0.000000  23.0    55.0\n",
      "32      0.000000   0.0     0.0\n",
      "33      0.000000   0.0     5.0\n",
      "34      0.000000   0.0    10.0\n",
      "35      0.000000   0.0    15.0\n",
      "\n",
      " Target Shape: (24, 1)\n",
      "    GlucoseValue\n",
      "19      0.328580\n",
      "20      0.200504\n",
      "21      0.399734\n",
      "22      0.670117\n",
      "23      0.912040\n"
     ]
    }
   ],
   "source": [
    "# load first file for ohio_training_559\n",
    "encoder_path = get_first_file(os.path.join(ohio_validation_directory, 'ohio_validation_544', 'EncoderSlices'))\n",
    "decoder_path = get_first_file(os.path.join(ohio_validation_directory, 'ohio_validation_544', 'DecoderSlices'))\n",
    "target_path = get_first_file(os.path.join(ohio_validation_directory, 'ohio_validation_544', 'TargetSlices'))\n",
    "\n",
    "encoder_tensor = torch.load(encoder_path)\n",
    "decoder_tensor = torch.load(decoder_path)\n",
    "target_tensor = torch.load(target_path)\n",
    "\n",
    "encoder_df = pd.DataFrame(encoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "decoder_df = pd.DataFrame(decoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "target_df = pd.DataFrame(target_tensor.numpy(), columns=[\"GlucoseValue\"])\n",
    "\n",
    "print(f\"\\n Encoder Shape: {encoder_df.shape}\")\n",
    "print(encoder_df.tail())\n",
    "print(f\"\\n Decoder Shape: {decoder_df.shape}\")\n",
    "print(decoder_df.tail(30))\n",
    "print(f\"\\n Target Shape: {target_df.shape}\")\n",
    "print(target_df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#CONTENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. OHIO T1DM Test Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 552, Number of Glucose Measurements: 2364\n",
      "Patient ID: 540, Number of Glucose Measurements: 2896\n",
      "Patient ID: 559, Number of Glucose Measurements: 2514\n",
      "Patient ID: 544, Number of Glucose Measurements: 2716\n",
      "Patient ID: 588, Number of Glucose Measurements: 2791\n",
      "Patient ID: 570, Number of Glucose Measurements: 2745\n",
      "Patient ID: 563, Number of Glucose Measurements: 2570\n",
      "Patient ID: 596, Number of Glucose Measurements: 2743\n",
      "Patient ID: 591, Number of Glucose Measurements: 2760\n",
      "Patient ID: 584, Number of Glucose Measurements: 2665\n",
      "Patient ID: 567, Number of Glucose Measurements: 2389\n",
      "Patient ID: 575, Number of Glucose Measurements: 2590\n"
     ]
    }
   ],
   "source": [
    "for ptid, df in ohio_test_data.items():\n",
    "    print(f\"Patient ID: {ptid}, Number of Glucose Measurements: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ptid, df in ohio_test_data.items():\n",
    "    df = df.copy()\n",
    "    df['real_value_flag'] = 1\n",
    "    df['TimeDiff'] = df['timestamp'].diff().dt.total_seconds()\n",
    "\n",
    "    # Identify rows where TimeDiff is around 600 seconds (10 min)\n",
    "    mask = (df['TimeDiff'] > 595) & (df['TimeDiff'] < 605)\n",
    "    insert_rows = df[mask].copy()\n",
    "\n",
    "    if not insert_rows.empty:\n",
    "        # Modify new rows: set `real_value_flag = 0`, shift `DateTime`, and set `GlucoseValue = NaN`\n",
    "        insert_rows['real_value_flag'] = 0\n",
    "        insert_rows['timestamp'] -= pd.to_timedelta(5, unit='m')\n",
    "        insert_rows['GlucoseValue'] = np.nan\n",
    "\n",
    "        # Append new rows to the dataframe and sort\n",
    "        df = pd.concat([df, insert_rows]).sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Convert 'value' column to numeric before interpolation\n",
    "    df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "    df['GlucoseValue'] = df['value'].interpolate(method='linear')\n",
    "\n",
    "    df['Hour'] = df['timestamp'].dt.hour\n",
    "    df['Minute'] = df['timestamp'].dt.minute\n",
    "    df['TimeDiff'] = df['timestamp'].diff().dt.total_seconds()\n",
    "    df['TimeDiffFlag'] = df['TimeDiff'].apply(lambda x: 0 if x < 295 or x > 305 else 1)\n",
    "    df['RollingTimeDiffFlag'] = df['TimeDiffFlag'].rolling(window=96).sum()\n",
    "\n",
    "    # Drop first 96 rows due to NaN values\n",
    "    # df = df.iloc[95:].reset_index(drop=True)\n",
    "\n",
    "    # drop columns\n",
    "    df = df.drop(columns=['timestamp', 'value', 'TimeDiff', 'TimeDiffFlag', 'real_value_flag'])\n",
    "\n",
    "    ohio_test_data[ptid] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ptid, df in ohio_test_data.items():\n",
    "    df['GlucoseValue'] = (df['GlucoseValue'] - normalisation_mean) / normalisation_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptid_test_slice_dict = {}\n",
    "\n",
    "\n",
    "for ptid, df in ohio_test_data.items():\n",
    "    rolling_flag_array = df[\"RollingTimeDiffFlag\"].to_numpy()  # Convert to NumPy array for fast indexing\n",
    "    num_rows = len(df)\n",
    "    starting_index = 0\n",
    "\n",
    "    slice_list = []\n",
    "\n",
    "    while starting_index + slice_size <= num_rows:\n",
    "        if rolling_flag_array[starting_index + slice_size - 1] == slice_size:  # Use precomputed array\n",
    "            slice_df = df.iloc[starting_index:starting_index + slice_size].copy()\n",
    "            slice_df = slice_df.drop(columns='RollingTimeDiffFlag')\n",
    "            slice_list.append(slice_df)\n",
    "            starting_index += 1\n",
    "\n",
    "        else:\n",
    "            starting_index += 1\n",
    "        \n",
    "    ptid_test_slice_dict[ptid] = slice_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ptid, slice_list in ptid_test_slice_dict.items():\n",
    "\n",
    "    ptid_count = 0\n",
    "\n",
    "    encoder_dir = os.path.join(ohio_test_directory, f'ohio_test_{ptid}', 'EncoderSlices')\n",
    "    os.makedirs(encoder_dir, exist_ok=True)\n",
    "    decoder_dir = os.path.join(ohio_test_directory, f'ohio_test_{ptid}', 'DecoderSlices')\n",
    "    os.makedirs(decoder_dir, exist_ok=True)\n",
    "    target_dir = os.path.join(ohio_test_directory, f'ohio_test_{ptid}', 'TargetSlices')\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    for i, slice_df in enumerate(slice_list):\n",
    "        # Replace all instances of 'slice' with 'slice_df'\n",
    "        encoder_input = slice.iloc[:encoder_input_size]\n",
    "        target = slice.iloc[encoder_input_size:]['GlucoseValue']\n",
    "\n",
    "        decoder_input = slice_df.iloc[-decoder_input_size:].copy().reset_index(drop=True)\n",
    "        decoder_input.loc[decoder_input.index[start_token_size:], 'GlucoseValue'] = 0\n",
    "\n",
    "        encoder_path = os.path.join(encoder_dir, f'{ptid_count}.pt')\n",
    "        decoder_path = os.path.join(decoder_dir, f'{ptid_count}.pt')\n",
    "        target_path = os.path.join(target_dir, f'{ptid_count}.pt')\n",
    "\n",
    "        torch.save(torch.tensor(encoder_input.values, dtype=torch.float32), encoder_path)\n",
    "        torch.save(torch.tensor(decoder_input.values, dtype=torch.float32), decoder_path)\n",
    "        torch.save(torch.tensor(target.values, dtype=torch.float32), target_path)\n",
    "\n",
    "        ptid_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Encoder Shape: (72, 3)\n",
      "    GlucoseValue  Hour  Minute\n",
      "67     -1.108723   6.0     4.0\n",
      "68     -1.151415   6.0     9.0\n",
      "69     -1.108723   6.0    14.0\n",
      "70     -1.108723   6.0    19.0\n",
      "71     -1.208338   6.0    24.0\n",
      "\n",
      " Decoder Shape: (36, 3)\n",
      "    GlucoseValue  Hour  Minute\n",
      "6      -0.496802   5.0    42.0\n",
      "7      -0.525263   5.0    47.0\n",
      "8      -0.553725   5.0    52.0\n",
      "9      -0.567955   5.0    57.0\n",
      "10     -0.553725   6.0     2.0\n",
      "11     -0.525263   6.0     7.0\n",
      "12      0.000000   6.0    12.0\n",
      "13      0.000000   6.0    17.0\n",
      "14      0.000000   6.0    22.0\n",
      "15      0.000000   6.0    27.0\n",
      "16      0.000000   6.0    32.0\n",
      "17      0.000000   6.0    37.0\n",
      "18      0.000000   6.0    42.0\n",
      "19      0.000000   6.0    47.0\n",
      "20      0.000000   6.0    52.0\n",
      "21      0.000000   6.0    57.0\n",
      "22      0.000000   7.0     2.0\n",
      "23      0.000000   7.0     7.0\n",
      "24      0.000000   7.0    12.0\n",
      "25      0.000000   7.0    17.0\n",
      "26      0.000000   7.0    22.0\n",
      "27      0.000000   7.0    27.0\n",
      "28      0.000000   7.0    32.0\n",
      "29      0.000000   7.0    37.0\n",
      "30      0.000000   7.0    42.0\n",
      "31      0.000000   7.0    47.0\n",
      "32      0.000000   7.0    52.0\n",
      "33      0.000000   7.0    57.0\n",
      "34      0.000000   8.0     2.0\n",
      "35      0.000000   8.0     7.0\n",
      "\n",
      " Target Shape: (24, 1)\n",
      "    GlucoseValue\n",
      "19      0.328580\n",
      "20      0.200504\n",
      "21      0.399734\n",
      "22      0.670117\n",
      "23      0.912040\n"
     ]
    }
   ],
   "source": [
    "# load first file for ohio_training_559\n",
    "encoder_path = get_first_file(os.path.join(ohio_test_directory, 'ohio_test_540', 'EncoderSlices'))\n",
    "decoder_path = get_first_file(os.path.join(ohio_test_directory, 'ohio_test_540', 'DecoderSlices'))\n",
    "target_path = get_first_file(os.path.join(ohio_test_directory, 'ohio_test_540', 'TargetSlices'))\n",
    "\n",
    "encoder_tensor = torch.load(encoder_path)\n",
    "decoder_tensor = torch.load(decoder_path)\n",
    "target_tensor = torch.load(target_path)\n",
    "\n",
    "encoder_df = pd.DataFrame(encoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "decoder_df = pd.DataFrame(decoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "target_df = pd.DataFrame(target_tensor.numpy(), columns=[\"GlucoseValue\"])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n Encoder Shape: {encoder_df.shape}\")\n",
    "print(encoder_df.tail())\n",
    "print(f\"\\n Decoder Shape: {decoder_df.shape}\")\n",
    "print(decoder_df.tail(30))\n",
    "print(f\"\\n Target Shape: {target_df.shape}\")\n",
    "print(target_df.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#CONTENTS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
